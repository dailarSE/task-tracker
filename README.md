# Task Tracker Project

Это проект, разработанный с целью практики работы с микросервисами, Spring Boot (Kafka, Scheduler, Mail), Docker и CI/CD.

## Описание

Приложение представляет собой простой трекер задач с возможностью регистрации/авторизации пользователей, создания/редактирования/удаления задач и ежедневными email-уведомлениями о прогрессе.

## Архитектура

Проект реализован как набор микросервисов, взаимодействующих через REST API и брокер сообщений Kafka:

*   **task-tracker-backend**: Spring Boot приложение, реализующее REST API для работы с пользователями и задачами.
*   **task-tracker-frontend**: Набор статических файлов (HTML, CSS, JS) для пользовательского интерфейса.
*   **task-tracker-scheduler**: Spring Boot приложение, использующее Spring Scheduler для ежедневной отправки отчетов пользователям через Kafka.
*   **task-tracker-email-sender**: Spring Boot приложение, слушающее Kafka топик и отправляющее email-уведомления с помощью Spring Mail.

Для хранения данных используется PostgreSQL.

## Структура репозитория

Проект использует монорепозиторий со следующей структурой:

```
task-tracker/
├── task-tracker-backend/ # Бэкенд
├── task-tracker-email-sender/ # Сервиса рассылки
├── task-tracker-frontend/ # Статические файлы фронтенда
├── task-tracker-scheduler/ # Планировщик
├── .gitignore # Правила исключения файлов для Git
├── Jenkinsfile # Конфигурация CI/CD пайплайна для Jenkins
├── docker-compose.yml # Конфигурация окружения Docker Compose
├── pom.xml # Корневой (родительский) Maven POM для Java-модулей
└── README.md # Этот файл
```

## Сборка и запуск

tbd

## Observability Stack

Данный проект включает в себя комплексный стек обсервабилити для мониторинга состояния и производительности приложения, а также для трассировки запросов и централизованного сбора логов. Все сервисы стека обсервабилити запускаются вместе с основным приложением командой `docker-compose up -d`.

### Компоненты стека:

*   **OpenTelemetry Collector (`otel-collector`):**
    *   **Роль:** Центральный узел для сбора телеметрических данных (метрики, трейсы, **логи**) от сервисов приложения по протоколу OTLP (OpenTelemetry Protocol).
    *   **Конфигурация:** `observability/otel-collector-config.yaml`
    *   **Принимает OTLP-данные:** HTTP порт – `otel-collector:4318`
*   **Prometheus (`prometheus`):**
    *   **Роль:** База данных временных рядов для хранения и выполнения запросов к метрикам. Собирает (скрейпит) метрики с OpenTelemetry Collector.
    *   **Веб-интерфейс (UI):** `http://prometheus:9090`
    *   **Конфигурация:** `observability/prometheus.yml`
*   **Grafana (`grafana`):**
    *   **Роль:** Платформа для визуализации метрик (из Prometheus), трейсов (из Tempo) и **логов (из Loki)**. Позволяет создавать дашборды и исследовать данные.
    *   **Веб-интерфейс (UI):** `http://grafana:3000` (По умолчанию может быть настроен анонимный вход с правами администратора).
    *   **Источники данных (Data Sources):** Автоматически настраиваются для Prometheus, Tempo и **Loki** при первом запуске (через provisioning в `observability/grafana/provisioning/datasources/datasource.yaml`).
*   **Grafana Tempo (`tempo`):**
    *   **Роль:** Высокомасштабируемое хранилище для распределенных трейсов. Хранит трейсы, полученные от OpenTelemetry Collector.
    *   **Доступ:** Преимущественно через секцию "Explore" в Grafana с выбранным источником данных Tempo.
    *   **Порт API (для Grafana):** `tempo:3200`.
*   **Grafana Loki (`loki`):**
    *   **Роль:** Система агрегации логов, горизонтально масштабируемая и высокодоступная, индексирует метаданные логов (метки), а не их полное содержимое. Хранит логи, полученные от OpenTelemetry Collector.
    *   **Доступ:** Через секцию "Explore" в Grafana с выбранным источником данных Loki.
    *   **Порт API (для OTel Collector и Grafana):** `loki:3100`.
    *   **Конфигурация:** `observability/loki-config.yaml`

### Как использовать:

1.  **Запуск стека:**
    *   Убедитесь, что Docker и Docker Compose установлены.
    *   Из корневой директории проекта выполните: `docker-compose up -d`. Эта команда скачает необходимые образы (если их нет локально) и запустит все сервисы проекта, включая стек обсервабилити, в фоновом режиме.

2.  **Просмотр метрик:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел "Dashboards". Найдите импортированные дашборды (например, для JVM метрик) или создайте свои, используя источник данных `Prometheus`.
    *   Для более детального анализа или выполнения Ad-hoc запросов можно использовать веб-интерфейс Prometheus (`http://localhost:9090`).

3.  **Просмотр трейсов:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел `Explore` и выберите источник данных `Tempo`.
    *   Используйте поиск по TraceID, фильтры по имени сервиса (например, `service.name="task-tracker-backend"` в атрибутах), длительности или другим параметрам.
    *   **Корреляция с логами:** Из интерфейса просмотра трейса в Tempo (в Grafana) должна быть доступна функция перехода к логам, связанным с выбранным трейсом или спаном.

4.  **Просмотр логов:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел `Explore` и выберите источник данных `Loki`.
    *   Используйте язык запросов LogQL для поиска и фильтрации логов.
    *   **Корреляция с трейсами:** Если лог содержит `traceid`, из интерфейса просмотра логов в Loki (в Grafana) должна быть доступна функция перехода к соответствующему трейсу в Tempo.
    *   **Локальные логи (для отладки):** Логи сервисов приложения (например, `task-tracker-backend`) также выводятся в консоль в формате JSON ECS.


### Конфигурация Backend-сервиса для Обсервабилити (`task-tracker-backend`)

Сервис `task-tracker-backend` сконфигурирован для интеграции со стеком обсервабилити через файл `application.yml` (и его профильные варианты). Ключевые аспекты:

*   **Имя Сервиса:** Задается через свойство `otel.service.name` (обычно по умолчанию используется значение `spring.application.name`). Это имя будет использоваться как атрибут `service.name` в трейсах, метриках и логах.
*   **Экспорт OTLP:**
    *   **Endpoint:** Настраивается через `otel.exporter.otlp.endpoint` (для трейсов, метрик и логов) для отправки данных в `otel-collector:4318` (HTTP).
*   **Сэмплинг Трейсов:** Управляется свойством `otel.traces.sampler.arg` (например, `1.0` для сэмплирования всех трейсов в dev-окружении, `0.1` для 10% в prod).
*   **Логирование:**
    *   **Формат в консоли:** Установлен в JSON ECS через `logging.structured.format.console: ecs` в `application.yml` (для удобства локального просмотра).
    *   **Экспорт в OTel Collector:** Логи автоматически захватываются и отправляются в OTel Collector.
    *   **Включение Trace ID / Span ID:** При активной трассировке, `trace_id` и `span_id` автоматически включаются в экспортируемые логи и доступны в Loki.
*   **Авто-инструментация OpenTelemetry:** Включена для Spring Web (HTTP-запросы), JDBC (взаимодействие с БД), Logback (логи), Micrometer (метрики) благодаря `opentelemetry-spring-boot-starter` и соответствующим свойствам в `otel.instrumentation.*`.

## Стратегия ветвления

Проект использует **GitHub Flow** в качестве стратегии ветвления:

1.  Ветка `main` всегда должна быть стабильной и готовой к деплою.
2.  Для работы над новой задачей или фиксом создается **новая ветка от `main`**. Название ветки должно быть описательным (например, `feature/user-registration`, `fix/auth-bug`).
3.  Работа ведется в этой новой ветке.
4.  Когда задача выполнена и протестирована локально, изменения пушатся в удаленную ветку на GitHub.
5.  Создается **Pull Request (PR)** из feature-ветки в `main`.
6.  PR проходит ревью.
7.  После успешного ревью и прохождения CI/CD пайплайна, PR мержится в `main`. Feature-ветка после мержа может быть удалена.

Коммиты в ветку `main` напрямую запрещены.

## CI/CD

Для автоматической сборки, тестирования и деплоя (на более поздних этапах) используется **Jenkins CI/CD Pipeline**, сконфигурированный в файле `Jenkinsfile`.

tbd

## dev/qa/prod

tbd
# Task Tracker Project

Это проект, разработанный с целью практики работы с микросервисами, Spring Boot (Kafka, Scheduler, Mail), Docker и CI/CD.

## Описание

Приложение представляет собой простой трекер задач с возможностью регистрации/авторизации пользователей, создания/редактирования/удаления задач и ежедневными email-уведомлениями о прогрессе.

## Архитектура

Проект реализован как набор микросервисов, взаимодействующих через REST API и брокер сообщений Kafka:

*   **task-tracker-backend**: Spring Boot приложение, реализующее REST API для работы с пользователями и задачами.
*   **task-tracker-frontend**: Набор статических файлов (HTML, CSS, JS) для пользовательского интерфейса.
*   **task-tracker-scheduler**: Spring Boot приложение, использующее Spring Scheduler для ежедневной отправки отчетов пользователям через Kafka.
*   **task-tracker-email-sender**: Spring Boot приложение, слушающее Kafka топик и отправляющее email-уведомления с помощью Spring Mail.

Для документирования архитектурных решений и визуализации структуры системы используются следующие подходы:

### Architecture Decision Records (ADR)

Ключевые архитектурные решения, принятые в ходе разработки проекта, фиксируются в формате Architecture Decision Records (ADR). Это помогает отслеживать историю принятия решений, их контекст, рассмотренные альтернативы и последствия.

Все ADR хранятся в директории [`docs/adr`](./docs/adr/) данного репозитория. Они сгруппированы по тематическим поддиректориям для удобства навигации. Файлы именуются с использованием временной метки для обеспечения хронологического порядка: `YYYY-MM-DD-HH-MM-описание-решения.md`.

**Основные тематические разделы ADR:**
*   [`docs/adr/common/`](./docs/adr/common/) - Общие архитектурные решения, затрагивающие весь проект.
*   [`docs/adr/backend-service/`](./docs/adr/backend-service/) - Решения, специфичные для основного бэкенд-сервиса.
*   [`docs/adr/database/`](./docs/adr/database/) - Решения, связанные с базами данных и персистентностью.
*   [`docs/adr/messaging/`](./docs/adr/messaging/) - Решения по брокерам сообщений и асинхронному взаимодействию.
*   [`docs/adr/observability/`](./docs/adr/observability/) - Решения по стеку мониторинга, логирования и трассировки.
*   [`docs/adr/ci-cd/`](./docs/adr/ci-cd/) - Решения по непрерывной интеграции и доставке.

### Архитектурные Диаграммы

Визуальное представление архитектуры системы реализуется с помощью диаграмм, преимущественно в нотации C4 Model. Исходные файлы диаграмм (в формате `.drawio`) и их экспортированные версии (например, `.webp`) хранятся в директории [`docs/diagrams`](./docs/diagrams/).

**Основные диаграммы:**
*   **C4 Level 1: System Context Diagram**
    *   Описание: Показывает систему "Task Tracker" как черный ящик и ее взаимодействие с пользователями и внешними системами.
    *   Исходник: [`docs/diagrams/c4-L1-system-context.drawio`](./docs/diagrams/c4-L1-system-context.drawio)
    *   Просмотр: [`docs/diagrams/c4-L1-system-context.webp`](./docs/diagrams/c4-L1-system-context.webp)
*   **C4 Level 2: Container Diagram (для "Системы Планировщика Задач")**
    *   Описание: Детализирует основные "контейнеры" (приложения, хранилища данных, брокеры сообщений) внутри системы "Task Tracker" и их взаимодействия.
    *   Исходник: [`docs/diagrams/task-tracker-system/c4-L2-containers.drawio`](./docs/diagrams/task-tracker-system/c4-L2-containers.drawio)
    *   Просмотр: [`docs/diagrams/task-tracker-system/c4-L2-containers.webp`](./docs/diagrams/task-tracker-system/c4-L2-containers.webp)


## Структура репозитория

Проект использует монорепозиторий со следующей структурой:

```
task-tracker/
├── docs/ # Документация
├── task-tracker-backend/ # Бэкенд
├── task-tracker-email-sender/ # Сервиса рассылки
├── task-tracker-frontend/ # Статические файлы фронтенда
├── task-tracker-scheduler/ # Планировщик
├── .gitignore # Правила исключения файлов для Git
├── Jenkinsfile # Конфигурация CI/CD пайплайна для Jenkins
├── docker-compose.yml # Конфигурация окружения Docker Compose
├── pom.xml # Корневой (родительский) Maven POM для Java-модулей
└── README.md # Этот файл
```

## Сборка и запуск

tbd

## Observability Stack

Данный проект включает в себя комплексный стек обсервабилити для мониторинга состояния и производительности приложения, а также для трассировки запросов и централизованного сбора логов. Все сервисы стека обсервабилити запускаются вместе с основным приложением командой `docker-compose up -d`.

### Компоненты стека:

*   **OpenTelemetry Collector (`otel-collector`):**
    *   **Роль:** Центральный узел для сбора телеметрических данных (метрики, трейсы, **логи**) от сервисов приложения по протоколу OTLP (OpenTelemetry Protocol).
    *   **Конфигурация:** `observability/otel-collector-config.yaml`
    *   **Принимает OTLP-данные:** HTTP порт – `otel-collector:4318`
*   **Prometheus (`prometheus`):**
    *   **Роль:** База данных временных рядов для хранения и выполнения запросов к метрикам. Собирает (скрейпит) метрики с OpenTelemetry Collector.
    *   **Веб-интерфейс (UI):** `http://prometheus:9090`
    *   **Конфигурация:** `observability/prometheus.yml`
*   **Grafana (`grafana`):**
    *   **Роль:** Платформа для визуализации метрик (из Prometheus), трейсов (из Tempo) и логов (из Loki). Позволяет создавать дашборды и исследовать данные.
    *   **Веб-интерфейс (UI):** `http://grafana:3000` (По умолчанию может быть настроен анонимный вход с правами администратора).
    *   **Источники данных (Data Sources):** Автоматически настраиваются для Prometheus, Tempo и Loki при первом запуске (через provisioning в `observability/grafana/provisioning/datasources/datasource.yaml`).
*   **Grafana Tempo (`tempo`):**
    *   **Роль:** Высокомасштабируемое хранилище для распределенных трейсов. Хранит трейсы, полученные от OpenTelemetry Collector.
    *   **Доступ:** Преимущественно через секцию "Explore" в Grafana с выбранным источником данных Tempo.
    *   **Порт API (для Grafana):** `tempo:3200`.
*   **Grafana Loki (`loki`):**
    *   **Роль:** Система агрегации логов, горизонтально масштабируемая и высокодоступная, индексирует метаданные логов (метки), а не их полное содержимое. Хранит логи, полученные от OpenTelemetry Collector.
    *   **Доступ:** Через секцию "Explore" в Grafana с выбранным источником данных Loki.
    *   **Порт API (для OTel Collector и Grafana):** `loki:3100`.
    *   **Конфигурация:** `observability/loki-config.yaml`

### Как использовать:

1.  **Запуск стека:**
    *   Убедитесь, что Docker и Docker Compose установлены.
    *   Из корневой директории проекта выполните: `docker-compose up -d`. Эта команда скачает необходимые образы (если их нет локально) и запустит все сервисы проекта, включая стек обсервабилити, в фоновом режиме.

2.  **Просмотр метрик:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел "Dashboards". Найдите импортированные дашборды (например, для JVM метрик) или создайте свои, используя источник данных `Prometheus`.
    *   Для более детального анализа или выполнения Ad-hoc запросов можно использовать веб-интерфейс Prometheus (`http://localhost:9090`).

3.  **Просмотр трейсов:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел `Explore` и выберите источник данных `Tempo`.
    *   Используйте поиск по TraceID, фильтры по имени сервиса (например, `service.name="task-tracker-backend"` в атрибутах), длительности или другим параметрам.
    *   **Корреляция с логами:** Из интерфейса просмотра трейса в Tempo (в Grafana) должна быть доступна функция перехода к логам, связанным с выбранным трейсом или спаном.

4.  **Просмотр логов:**
    *   Откройте Grafana (`http://localhost:3000`).
    *   Перейдите в раздел `Explore` и выберите источник данных `Loki`.
    *   Используйте язык запросов LogQL для поиска и фильтрации логов.
    *   **Корреляция с трейсами:** Если лог содержит `traceid`, из интерфейса просмотра логов в Loki (в Grafana) должна быть доступна функция перехода к соответствующему трейсу в Tempo.
    *   **Локальные логи (для отладки):** Логи сервисов приложения (например, `task-tracker-backend`) также выводятся в консоль в формате JSON ECS.


### Конфигурация Backend-сервиса для Обсервабилити (`task-tracker-backend`)

Сервис `task-tracker-backend` сконфигурирован для интеграции со стеком обсервабилити через файл `application.yml` (и его профильные варианты). Ключевые аспекты:

*   **Имя Сервиса:** Задается через свойство `otel.service.name` (обычно по умолчанию используется значение `spring.application.name`). Это имя будет использоваться как атрибут `service.name` в трейсах, метриках и логах.
*   **Экспорт OTLP:**
    *   **Endpoint:** Настраивается через `otel.exporter.otlp.endpoint` (для трейсов, метрик и логов) для отправки данных в `otel-collector:4318` (HTTP).
*   **Сэмплинг Трейсов:** Управляется свойством `otel.traces.sampler.arg` (например, `1.0` для сэмплирования всех трейсов в dev-окружении, `0.1` для 10% в prod).
*   **Логирование:**
    *   **Формат в консоли:** Установлен в JSON ECS через `logging.structured.format.console: ecs` в `application.yml` (для удобства локального просмотра).
    *   **Экспорт в OTel Collector:**  Настроен через автоконфигурируемый OTel Logback Appender (`otel.instrumentation.logback-appender.enabled: true`). Логи автоматически захватываются и отправляются в OTel Collector в формате OTel Log Data Model.
    *   **Включение Trace ID / Span ID:** При активной трассировке, `trace_id` и `span_id` автоматически включаются в экспортируемые логи и доступны в Loki.
*   **Авто-инструментация OpenTelemetry:** Включена для Spring Web (HTTP-запросы), JDBC (взаимодействие с БД), Logback (логи), Micrometer (метрики) благодаря `opentelemetry-spring-boot-starter` и соответствующим свойствам в `otel.instrumentation.*`.

## Стратегия ветвления

Проект использует **GitHub Flow** в качестве стратегии ветвления:

1.  Ветка `main` всегда должна быть стабильной и готовой к деплою.
2.  Для работы над новой задачей или фиксом создается **новая ветка от `main`**. Название ветки должно быть описательным (например, `feature/user-registration`, `fix/auth-bug`).
3.  Работа ведется в этой новой ветке.
4.  Когда задача выполнена и протестирована локально, изменения пушатся в удаленную ветку на GitHub.
5.  Создается **Pull Request (PR)** из feature-ветки в `main`.
6.  PR проходит ревью.
7.  После успешного ревью и прохождения CI/CD пайплайна, PR мержится в `main`. Feature-ветка после мержа может быть удалена.

Коммиты в ветку `main` напрямую запрещены.

## CI/CD

Проект использует Jenkins для непрерывной интеграции (CI). Пайплайн CI/CD сконфигурирован в файле `Jenkinsfile` в корне репозитория и выполняет следующие ключевые задачи:

1.  **Сборка проекта:** Используется Maven для сборки всех модулей проекта (`mvn clean verify`).
2.  **Тестирование:**
    *   **Юнит-тесты:** Автоматически запускаются с помощью Maven Surefire Plugin.
    *   **Интеграционные тесты:** Автоматически запускаются с помощью Maven Failsafe Plugin. Для тестов слоя персистентности используются Testcontainers с PostgreSQL.
    *   **Разделение:** Юнит- и интеграционные тесты выполняются как отдельные этапы в жизненном цикле Maven.
3.  **Анализ качества кода:**
    *   **Покрытие кода:** Собирается статистика покрытия кода с помощью JaCoCo. Отчеты генерируются для каждого модуля и публикуются в Jenkins. Это позволяет отслеживать, какая часть кода покрыта тестами.
4.  **Публикация результатов:**
    *   **Результаты тестов:** JUnit-отчеты (как для юнит-, так и для интеграционных тестов) публикуются в Jenkins, предоставляя детализацию по выполненным тестам.
    *   **Отчеты о покрытии:** Отчеты JaCoCo (включая HTML-представление с подсветкой покрытия исходного кода) публикуются в Jenkins.
5.  **Артефакты:** Собранные JAR-файлы архивируются.

**Ключевые технологии и подходы в CI:**
*   **Jenkins:** Основной CI-сервер.
*   **Jenkins Pipeline (Declarative):** Пайплайн определен в `Jenkinsfile`.
*   **Внешний Jenkins агент:** Сборки выполняются на выделенном агенте с доступом к Docker (для Testcontainers).
*   **Maven:** Система сборки и управления зависимостями.
*   **JaCoCo:** Инструмент для измерения покрытия кода тестами.

Более детальная информация о конфигурации CI-сервера, агента и пайплайна задокументирована в [**ADR-0010: Выбор и конфигурация CI-сервера и агента**](./docs/adr/ci-cd/2025-05-09-00-00-выбор-и-конфигурация-ci-сервера-и-агента.md), а конвенции тестирования и отчетности — в [**ADR-0016: Конвенции и инструменты для интеграционного тестирования JPA и CI-отчетности**](./docs/adr/common/2025-05-10-14-15-конвенции-jpa-тестирования-и-ci-отчетности.md).

*(Задачи по Continuous Deployment (CD) будут реализованы на более поздних этапах проекта.)*

## dev/qa/prod

tbd
services:
  task-tracker-frontend:
    image: nginx:1.28.0-alpine
    container_name: task-tracker-frontend-nginx
    ports:
      - "80:80"
    volumes:
      - ./task-tracker-frontend:/usr/share/nginx/html:ro
    networks:
      - task-tracker-net
    restart: unless-stopped
#    depends_on:
#      - task-tracker-backend
  postgres:
    image: ${POSTGRES_IMAGE:-postgres:17.4}
    
    container_name: task-tracker-postgres
    environment:
      POSTGRES_DB: task_tracker_db
      POSTGRES_USER_FILE: /run/secrets/postgres_user
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - task-tracker-net
    restart: unless-stopped
    secrets:
      - postgres_user
      - postgres_password
   
  kafka:
    image: bitnami/kafka:4.0.0 # Используем образ с поддержкой KRaft
    container_name: task-tracker-kafka

    ports:
      # Пробрасываем порт 9092 на хост для доступа с localhost:9092.
      # Этот внешний порт будет мапиться на ВНУТРЕННИЙ порт 9094 (слушатель EXTERNAL).
      - '9092:9094'
      # Внутренний порт 9092 (слушатель INTERNAL) доступен внутри Docker сети по имени сервиса 'kafka:9092'.
      # Внутренний порт 9093 (слушатель CONTROLLER) доступен внутри Docker сети по имени сервиса 'kafka:9093' для KRaft.
      # Контроллер порт 9093 не пробрасываем на хост.

    networks:
      - task-tracker-net
    volumes:
      # Том для сохранения данных и метаданных KRaft между запусками
      - kafkadata:/opt/bitnami/kafka/data

    environment:
      # --- Настройки KRaft ---
      KAFKA_ENABLE_KRAFT: 'yes' # Включаем режим KRaft
      KAFKA_BROKER_ID: 1        # Уникальный ID брокера
      KAFKA_NODE_ID: 1          # Уникальный ID узла KRaft (совпадает с Broker ID для одного узла)
      # Роль процесса KRaft: этот узел является и брокером, и контроллером
      KAFKA_CFG_PROCESS_ROLES: broker,controller

      # --- Настройка слушателей и рекламируемых адресов ---
      # Настройка слушателей ВНУТРИ контейнера, привязанных к РАЗНЫМ портам
      # INTERNAL://:9092   - Для трафика внутри Docker сети
      # CONTROLLER://:9093 - Для внутренней коммуникации KRaft
      # EXTERNAL://:9094   - Для трафика с хост-машины через проброшенный порт 9092->9094
      KAFKA_LISTENERS: INTERNAL://:9092,CONTROLLER://:9093,EXTERNAL://:9094

      # Настройка РЕКЛАМИРУЕМЫХ адресов - как клиенты должны подключаться
      # INTERNAL://kafka:9092    - Адрес для клиентов внутри Docker сети (имя сервиса)
      # EXTERNAL://localhost:9092 - Адрес для клиентов на хост-машине (localhost и проброшенный порт 9092)
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:9092

      # Маппинг протоколов безопасности к слушателям (для PLAINTEXT просто указываем PLAINTEXT)
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT

      # Указываем, какой слушатель используется для коммуникации между брокерами (для 1 узла это формальность)
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # Имя слушателя контроллера
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # Информация о кворуме контроллеров (для 1 узла - он сам).
      # Формат: <NodeId>@<Hostname>:<Port>
      # Используем внутреннее имя сервиса 'kafka' и внутренний порт контроллера 9093
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # --- Прочее ---
      # Разрешаем нешифрованный трафик для простоты в dev (специфично для Bitnami)
      #TODO принять решение о TLS в продакш реди среде
      ALLOW_PLAINTEXT_LISTENER: 'yes'

      # Задержка ребалансировки групп консьюмеров при старте (для dev ускоряем)
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

      # Директория, где Kafka будет хранить данные. Должна совпадать с маппингом тома.
      KAFKA_STORAGE_DIRS: /opt/bitnami/kafka/data

      # Фактор репликации для топика смещений консьюмеров (важно для стабильности в 1-узловом кластере)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

      # Автоматическое создание топика email_commands при старте (имя:партиции:репликации) - удобно для dev
      KAFKA_CREATE_TOPICS_EXTRA: "task_tracker.notifications.email_commands:1:1"
    # Bitnami образ сам форматирует хранилище KRaft при первом запуске, если директория пуста.
    # Не требуется сложная 'command' секция как у Confluent.
    restart: unless-stopped

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.123.0
    container_name: otel-collector
    command:
      - "--config=/etc/otelcol-contrib/config.yaml"
    volumes:
      - ./observability/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml # Мапим конфиг
    ports:
      - "4318:4318" # OTLP HTTP receiver
      - "8889:8889" # Prometheus exporter port (для сбора метрик самим Prometheus)
    networks:
      - task-tracker-net
    depends_on:
      - tempo
      - loki

  prometheus:
    image: prom/prometheus:v3.3.1 
    container_name: prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      -  "--web.enable-lifecycle"
      - "--web.enable-remote-write-receiver"
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml # Мапим конфиг
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - task-tracker-net
    depends_on:
      - otel-collector
  tempo-init:
    image: grafana/tempo:main-2345116
    container_name: tempo-init
    user: root
    volumes:
      - tempo_data:/tmp/tempo
    # Явно указываем entrypoint и команду
    entrypoint: ["chown"]
    command: ["-R", "10001:10001", "/tmp/tempo"]
  tempo:
    image: grafana/tempo:main-2345116
    container_name: tempo
    command: ["-config.file=/etc/tempo/tempo-config.yaml"]
    volumes:
      - ./observability/tempo-config.yaml:/etc/tempo/tempo-config.yaml
      - tempo_data:/tmp/tempo
    ports:
      - "3200:3200" # Tempo API/UI port
    networks:
      - task-tracker-net
    depends_on:
      tempo-init:
        condition: service_completed_successfully

  grafana:
    image: grafana/grafana:12.0.0
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      # Мапим папку для автоматического провижининга Data Sources
      - ./observability/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true # Разрешить анонимный доступ для простоты
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin # Дать админские права анониму
    ports:
      - "3000:3000" # UI Grafana
    networks:
      - task-tracker-net
    depends_on:
      - prometheus
      - tempo
      - loki

  loki:
    image: grafana/loki:main-23c4f8d
    container_name: loki
    command: [ "-config.file=/etc/loki/local-config.yaml" ]
    ports:
      - "3100:3100" # HTTP port Loki
    volumes:
      - ./observability/loki-config.yaml:/etc/loki/local-config.yaml # Мапим конфиг Loki
      - loki_data:/loki
    networks:
      - task-tracker-net

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console
    container_name: redpanda-console
    restart: unless-stopped
    ports:
      - "8081:8080"
    networks:
      - task-tracker-net
    depends_on:
      - kafka # Запускать после Kafka
    environment:
      KAFKA_BROKERS: "kafka:9092"
      # Если у вас несколько брокеров, перечислите их через запятую: "kafka1:9092,kafka2:9092"

      # Опционально: конфигурация для Schema Registry, если используете
      # KAFKA_SCHEMAREGISTRY_ENABLED: "true"
      # KAFKA_SCHEMAREGISTRY_URLS: "http://schema-registry:8081"

      # Конфигурация самой консоли (можно оставить по умолчанию)
      CONSOLE_CONFIG_FILEPATH: "" # Путь к файлу конфигурации внутри контейнера, если используется
      CONSOLE_LOG_LEVEL: "info"

volumes:
  pgdata:
  kafkadata:
  prometheus_data:
  grafana_data:
  tempo_data:
  loki_data:

networks:
  task-tracker-net:
    driver: bridge

secrets:
  postgres_user:
    file: ./secrets/.postgres_user 
  postgres_password:
    file: ./secrets/.postgres_password